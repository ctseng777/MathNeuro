import os
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--model', help="Huggingface model to train, entered as string", type = str)
parser.add_argument('--eval_datasets', nargs='+', help="dataset(s) to evaluate models on post pruning to evalaute catastrophic forgetting, entered as strings; should be task names from Eleuther AI LM Evaluation Harness", type = str)
parser.add_argument('--train_dataset', help="path to math train dataset; should be a path to a CSV file with question/solution pairs in a columns titled 'question' and 'solution' along with ground-truth answers in a column called 'answer'", type = str)
parser.add_argument('--calibration_datasets', nargs='+', help="path to calibration datasets; should be paths to CSV files with instruction/response pairs in a column titled 'qa'", type = str)
parser.add_argument('--save_path', help="save path for eval results after running Eleuther AI LM Evaluation Harness post pruning", type = str)
parser.add_argument('--text_file', help="name of text file for saving pruning results during training if evaluating math reasoning using a non-Eleuther AI LM Evaluation Harness task in a PoT format", type = str)
parser.add_argument('--num_repeats', help="number of repeats for pruning or scaling experiment", type = int, default = 5)
parser.add_argument('--pre_train_eval', help="bool to indicate if full evaluation on eval and train datasets should be conducted before training", action="store_true")
parser.add_argument('--random_state', help="random state for initial dataset shuffling and creating train/eval split for train dataset", type = int, default = 42)
parser.add_argument('--scalar', help="scale factor for top parameters; default is 0 to run pruning experiments", type = float, default = 0)
parser.add_argument('--eval_dataset_size', help="desired number of samples for task specific eval dataset", type = int, default = None)
parser.add_argument('--eval_dataset_subset', help="desired number of samples for task specific eval dataset if subsetting to reduce run time", type = int, default = 100)
parser.add_argument('--train_dataset_size', help="desired number of samples to use from the training dataset CSV", type = int, default = None)
parser.add_argument('--calibration_dataset_size', help="desired number of samples to use from each calibration dataset CSV", type = int, default = None)
parser.add_argument('--calibration_dataset_names', nargs='+', help="desired name of calibration datasets; should be strings entered in same order as calibration_datasets", type = str)
parser.add_argument('--num_samples', help="desired number of samples for calculating task specific parameters", type = int, default = 500)
parser.add_argument('--train_lm_eval_task', nargs='?', help="if your training dataset is an Eleuther AI LM Evaluation Harness task, specify the associated task for the test set.", type = str, default = None)
parser.add_argument('--proportion', help="desired proportion of top parameters to calculate", type = float, default = None)
parser.add_argument('--streetmath_eval', help="run StreetMath evaluation using the current model weights", action="store_true")
parser.add_argument('--streetmath_jsonl', help="path to StreetMath JSONL file", type = str, default = None)
parser.add_argument('--streetmath_root', help="path to StreetMathDataset root (contains streetmath_benchmark/ and data/)", type = str, default = None)
parser.add_argument('--streetmath_limit', help="limit StreetMath samples", type = int, default = None)
parser.add_argument('--streetmath_max_tokens', help="max new tokens for StreetMath generation", type = int, default = 256)
parser.add_argument('--streetmath_temperature', help="temperature for StreetMath generation", type = float, default = 0.2)
parser.add_argument('--streetmath_top_p', help="top_p for StreetMath generation", type = float, default = None)
parser.add_argument('--streetmath_top_k', help="top_k for StreetMath generation", type = int, default = None)
parser.add_argument('--streetmath_no_system', help="disable StreetMath system prompt", action="store_true")
parser.add_argument('--streetmath_custom_system', help="custom StreetMath system prompt text", type = str, default = None)
parser.add_argument('--streetmath_no_tools', help="disallow tool calls in StreetMath prompts", action="store_true")
parser.add_argument('--streetmath_hint', help="add StreetMath hint block", action="store_true")
parser.add_argument('--streetmath_prompt_template_file', help="path to custom StreetMath prompt template", type = str, default = None)
args = parser.parse_args()
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import re
import lm_eval
import json 
from pathlib import Path
import sys
import time

def read_csv_safe(path, random_state=None):
    df = pd.read_csv(path, engine="python", on_bad_lines="skip")
    if random_state is None:
        return df
    return df.sample(frac=1, random_state=random_state)


def build_task_manager(task_names):
    tasks_root = Path(lm_eval.__file__).resolve().parent / "tasks"
    task_dirs = set()
    for name in task_names:
        if not name:
            continue
        if name == "mmlu" or name.startswith("mmlu_"):
            task_dirs.add(tasks_root / "mmlu")
        elif name.startswith("gsm8k"):
            task_dirs.add(tasks_root / "gsm8k")
        else:
            candidate = tasks_root / name
            if candidate.is_dir():
                task_dirs.add(candidate)
    if task_dirs:
        return lm_eval.tasks.TaskManager(
            include_path=[str(path) for path in sorted(task_dirs)],
            include_defaults=False,
        )
    return lm_eval.tasks.TaskManager()


def _streetmath_full_prompt(tokenizer, user_prompt, system_prompt):
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_prompt})
    apply = getattr(tokenizer, "apply_chat_template", None)
    if callable(apply):
        try:
            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        except Exception:
            pass
    return (system_prompt + "\n\n" if system_prompt else "") + user_prompt


def run_streetmath_eval(model, tokenizer, output_path, args):
    if args.streetmath_root:
        streetmath_root = Path(args.streetmath_root).resolve()
    else:
        repo_root = Path(__file__).resolve().parents[1]
        streetmath_root = repo_root / "StreetMathDataset"
    if str(streetmath_root) not in sys.path:
        sys.path.insert(0, str(streetmath_root))

    from streetmath_benchmark.loader import load_streetmath
    from streetmath_benchmark.prompt import build_prompt, DEFAULT_SYSTEM_PROMPT
    from streetmath_benchmark.eval import build_result_record, summarize

    if args.streetmath_prompt_template_file:
        try:
            custom_user = Path(args.streetmath_prompt_template_file).read_text(encoding="utf-8")
        except Exception:
            custom_user = None
    else:
        custom_user = None

    if args.streetmath_no_system:
        system_prompt = None
    else:
        system_prompt = args.streetmath_custom_system or DEFAULT_SYSTEM_PROMPT

    if args.streetmath_jsonl:
        streetmath_jsonl = args.streetmath_jsonl
    else:
        streetmath_jsonl = str(streetmath_root / "data" / "street_math_test.jsonl")

    samples = load_streetmath(
        local_jsonl=streetmath_jsonl,
        split="test",
        limit=None,
        shuffle=False,
        seed=args.random_state,
    )
    if args.streetmath_limit is not None:
        limit = int(args.streetmath_limit)
        if limit > 0 and len(samples) > limit:
            stride = max(1, len(samples) // limit)
            samples = samples[::stride][:limit]

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("")
    results = []
    for sample in samples:
        user_prompt = build_prompt(
            sample=sample,
            custom_instructions=custom_user,
            disallow_tools=args.streetmath_no_tools,
            hint=args.streetmath_hint,
        )
        full_prompt = _streetmath_full_prompt(tokenizer, user_prompt, system_prompt)

        if getattr(tokenizer, "pad_token", None) is None and getattr(tokenizer, "eos_token", None) is not None:
            try:
                tokenizer.pad_token = tokenizer.eos_token
            except Exception:
                pass

        start = time.time()
        tok = tokenizer(full_prompt, return_tensors="pt").to(model.device)
        gen_kwargs = {
            **tok,
            "temperature": args.streetmath_temperature,
            "max_new_tokens": args.streetmath_max_tokens,
            "top_p": args.streetmath_top_p,
            "top_k": args.streetmath_top_k,
            "do_sample": args.streetmath_temperature > 0,
        }
        out = model.generate(**gen_kwargs)
        elapsed = time.time() - start
        out_text = tokenizer.decode(out[0], skip_special_tokens=True)
        prompt_text = tokenizer.decode(tok["input_ids"][0], skip_special_tokens=True)
        response_text = out_text[len(prompt_text):].strip()
        usage = {
            "prompt_tokens": int(tok["input_ids"].shape[-1]),
            "completion_tokens": int(out[0].shape[-1] - tok["input_ids"].shape[-1]),
            "total_tokens": int(out[0].shape[-1]),
            "token_count_source": "tokenizer",
        }

        rec = build_result_record(
            sample=sample,
            provider_name="transformers",
            model_name=args.model,
            response_text=response_text,
            usage=usage,
            elapsed=elapsed,
            prompt_text=full_prompt,
            eval_config={
                "decoding": {
                    "temperature": args.streetmath_temperature,
                    "top_p": args.streetmath_top_p,
                    "top_k": args.streetmath_top_k,
                    "max_tokens": args.streetmath_max_tokens,
                    "seed": args.random_state,
                },
            },
        )
        results.append(rec)
        with open(output_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    summary = summarize([r for r in results if "judgement" in r])
    with open(output_path, "a", encoding="utf-8") as f:
        f.write(json.dumps({"summary": summary}, ensure_ascii=False) + "\n")

if 'sgsm' in args.train_dataset:
    df = read_csv_safe(args.train_dataset) # Load SGSM dataset for few-shot prompting
    df = df[df['subset']=="sgsm_train"] # Subset SGSM to verified training subset
    df = df.sample(frac = 1, random_state = args.random_state)
    for i in range(0, len(df)):
        try:
            answer = df.iloc[i]['answer']
            answer = float(answer)
            df.iloc[i]['answer'] = answer
        except:
            df = df.drop([i])
    
    train = df.iloc[0:1500]
    
    val = df.iloc[1500:]
    val = val.sample(frac = 1, random_state = args.random_state)

if 'sgsm' not in args.train_dataset:
    train = read_csv_safe(args.train_dataset, random_state=args.random_state) # Load SGSM dataset for few-shot prompting

if args.train_dataset_size is not None:
    train = train.sample(n=min(args.train_dataset_size, len(train)), random_state=args.random_state)
    

calibration_datasets = []
for dataset in args.calibration_datasets:
    if '/' in dataset:
        dataset_name = dataset.split('/')[-1]
        dataset_name = dataset_name.split('.csv')[0]
        calibration_datasets.append(dataset_name)
    else:
        dataset_name = dataset.split('.csv')[0]
        calibration_datasets.append(dataset_name)

dataset_list = []
for dataset, dataset_name, name in zip(args.calibration_datasets, calibration_datasets, args.calibration_dataset_names):
    # Load the dataset into a DataFrame
    globals()[dataset_name] = read_csv_safe(dataset, random_state=args.random_state)  # Shuffle the DataFrame

    if args.calibration_dataset_size is not None:
        globals()[dataset_name] = globals()[dataset_name].sample(
            n=min(args.calibration_dataset_size, len(globals()[dataset_name])),
            random_state=args.random_state
        )
    
    # Assign a name attribute to the DataFrame
    globals()[dataset_name].name = name
    
    # Append the actual DataFrame object to the list
    dataset_list.append(globals()[dataset_name])

task_manager_names = []
if args.eval_datasets:
    task_manager_names.extend(args.eval_datasets)
if args.train_lm_eval_task:
    task_manager_names.append(args.train_lm_eval_task)
    
output_file = f"{args.save_path}/eval_results/{args.model}/{args.text_file}"
results_path =  f"{args.save_path}/eval_results/{args.model}/"
os.makedirs(os.path.dirname(results_path), exist_ok=True)

tokenizer = AutoTokenizer.from_pretrained(args.model)
model = AutoModelForCausalLM.from_pretrained(args.model, device_map="auto", torch_dtype=torch.bfloat16)
if args.pre_train_eval:
    if 'sgsm' in args.train_dataset:
        prune_solve = []
        prune_code = []
        prune_solutions = []
        for i in range(0, min(args.eval_dataset_subset, len(val))):
            # Format the prompt
            prompts = []
            questions = []
            final_question = val.iloc[i]['question']
            final_answer = val.iloc[i]['answer']
            final_prompt = f"""Instruct: {final_question} Let's write a Python program.\nOutput:"""
    
            for j in range(0, 8):
                question = train['question'].iloc[j]
                questions.append(question)
                answer = train['solution'].iloc[j]
                prompt = f"""Instruct: {question} Let's write a Python program.\nOutput:\n{answer}"""
                if prompt not in prompts:
                    prompts.append(prompt)
    
            prompts.append(final_prompt)
            formatted_prompt = "\n\n".join(prompts)
            #Query the model 
            inputs = tokenizer.encode(formatted_prompt, return_tensors="pt").to(model.device)
            model_answer = None
            output = model.generate(inputs, max_new_tokens = 150)
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
            # Split the generated text by the prompt to extract the newly generated part
            generated_text_parts = generated_text.split(final_prompt)
            solution_text = generated_text_parts[-1].strip()
            prune_solutions.append(solution_text)
            if "Instruct:" in solution_text:
                solution_text = solution_text.split("Instruct:")[0] # Split up a generation that contains more than one question
            if "print" in solution_text:
                solution_text = solution_text.split("print")[0] # Split up a generation that contains a print statement
            if "Student:" in solution_text:
                solution_text = solution_text.split("Student:")[0] # Split up a generation that contains more than one question
            if "Output:" in solution_text:
                solution_text = solution_text.split("Output:")[0] # Split up a generation that contains more than one question
            if "#TODO" in solution_text:
                solution_text = solution_text.split("#TODO")[0] # Split up a generation that contains more than one question
            #solutions.append(solution_text)
            if 'return result' in solution_text:
                # Split the string on 'return result' but keep 'return result' in the result
                parts = re.split(r'(return result)', solution_text)
    
                # Rejoin the parts correctly
                solution_text = parts[0] + parts[1]
            try:
                exec(solution_text)
                model_answer = solution()
                prune_code.append(1)
                model_answer = float(model_answer)
                if model_answer != final_answer:
                    prune_solve.append(0)
    
                if model_answer == final_answer:
                    prune_solve.append(1)
    
            except:
                prune_code.append(0)
                prune_solve.append(0)
    
        with open(output_file, "a") as f:  # Open the file in append mode ("a")
                f.write(f"Average eval accuracy on {min(args.eval_dataset_subset, len(val))} questions before training with greedy decoding (few-shot): {np.mean(prune_solve)}\n") 
        task_manager = build_task_manager(task_manager_names)
        #--log_samples --output_path results/phi_15_base --device cuda:0 --batch_size auto:4
        # Setting `task_manager` to the one above is optional and should generally be done
        # if you want to include tasks from paths other than ones in `lm_eval/tasks`.
        # `simple_evaluate` will instantiate its own task_manager if it is set to None here.
        results = lm_eval.simple_evaluate( # call simple_evaluate
            model = 'hf',
            model_args = {'pretrained':model, 'dtype': 'bfloat16', 'tokenizer': tokenizer},
            tasks=args.eval_datasets,
            task_manager=task_manager,
            log_samples = False, 
            batch_size = 'auto:4',
            limit = args.eval_dataset_subset,
            random_seed = args.random_state
        )
        results_path = f"{args.save_path}/eval_results/{args.model}/pre_results.json"
        os.makedirs(os.path.dirname(results_path), exist_ok=True)
        with open(results_path, "w") as outfile: 
            json.dump(results['results'], outfile)
        if args.streetmath_eval:
            streetmath_out = f"{args.save_path}/eval_results/{args.model}/STREET_MATH_pre.jsonl"
            run_streetmath_eval(model, tokenizer, streetmath_out, args)
    
    if args.train_lm_eval_task is not None:
        task_manager = build_task_manager(task_manager_names)
        #--log_samples --output_path results/phi_15_base --device cuda:0 --batch_size auto:4
        # Setting `task_manager` to the one above is optional and should generally be done
        # if you want to include tasks from paths other than ones in `lm_eval/tasks`.
        # `simple_evaluate` will instantiate its own task_manager if it is set to None here.
        results = lm_eval.simple_evaluate( # call simple_evaluate
            model = 'hf',
            model_args = {'pretrained':model, 'dtype': 'bfloat16', 'tokenizer': tokenizer},
            tasks=[args.train_lm_eval_task],
            task_manager=task_manager,
            log_samples = False, 
            batch_size = 'auto:4',
            limit = args.eval_dataset_subset, 
            random_seed = args.random_state
        )
        results_path = f"{args.save_path}/eval_results/{args.model}/pre_results_train_task.json"
        os.makedirs(os.path.dirname(results_path), exist_ok=True)
        with open(results_path, "w") as outfile: 
            json.dump(results['results'], outfile)
        
        results = lm_eval.simple_evaluate( # call simple_evaluate
            model = 'hf',
            model_args = {'pretrained':model, 'dtype': 'bfloat16', 'tokenizer': tokenizer},
            tasks=args.eval_datasets,
            task_manager=task_manager,
            log_samples = False, 
            batch_size = 'auto:4'
        )
        results_path = f"{args.save_path}/eval_results/{args.model}/pre_results.json"
        os.makedirs(os.path.dirname(results_path), exist_ok=True)
        with open(results_path, "w") as outfile: 
            json.dump(results['results'], outfile)
        if args.streetmath_eval:
            streetmath_out = f"{args.save_path}/eval_results/{args.model}/STREET_MATH_pre.jsonl"
            run_streetmath_eval(model, tokenizer, streetmath_out, args)
            
magnitude = {}
def getActivation(name):
    # The hook function
    def hook(module, input, output):
        activations = input[0]  # Get the input activations
        weights = module.weight.data  # Get the weights
        # Compute the norm of activations along dim=1
        activations_norm = activations.norm(p=2, dim=1).to(torch.bfloat16)
        # Multiply activations by the absolute value of weights
        modified_output = activations_norm * torch.abs(weights)
        magnitude[name] = modified_output.detach()  # Store the modified output
    # Return the hook function
    return hook

for name, module in model.named_modules():
    if (isinstance(module, (nn.Linear))):
        hook_fn = getActivation(name)  # Get the hook function
        module.register_forward_hook(hook_fn)  # Register the hook function

if 'bad_gens_full.csv' in args.calibration_datasets:
    def find_params(model, gens, keep_ratio, prune = True, largest = True, num_samples = len(bad_gens_full)):
        global chosen_params
        cuda_device = "cuda" if torch.cuda.is_available() else "cpu"

        param_dict = {}
        for name, param in model.named_parameters():
            param_dict[name] = torch.zeros_like(param).to(param.device)
        
        for i in range(0, num_samples):
            inputs = tokenizer.encode(gens.iloc[i]['0'], return_tensors="pt").to(model.device)
            outputs = model(inputs)
            for key, tensor in magnitude.items():
                try:
                    param_dict[f"{key}.weight"] += tensor
                except:
                    pass
        keys_to_remove = [key for key in param_dict if key.split('.weight')[0] not in magnitude]
    
        for key in keys_to_remove:
            del param_dict[key]
        
        mask_dict = {}
    
    
        for k, v in param_dict.items():
            if "embed" in k:
                if prune == False:
                    mask_dict[k] = torch.zeros_like(v).to(v.device)
                else:
                    mask_dict[k] = torch.ones_like(v).to(v.device)
    
            else:
                if prune == False:
                    sizes = v.shape
                    num_params = v.numel()
                    keep_num = int(num_params * keep_ratio)
                    tensor = v.view(-1)
                    top_pos = torch.topk(torch.abs(tensor), keep_num, largest = largest)[1]
                    mask_dict[k] = torch.zeros_like(tensor, device=tensor.device)
                    mask_dict[k][top_pos] = 1
                    mask_dict[k] = mask_dict[k].reshape(v.shape).to(tensor.device)
                else:
                    sizes = v.shape
                    num_params = v.numel()
                    keep_num = int(num_params * keep_ratio)
                    tensor = v.view(-1)
                    top_pos = torch.topk(torch.abs(tensor), keep_num, largest = largest)[1]
                    mask_dict[k] = torch.ones_like(tensor, device=tensor.device)
                    mask_dict[k][top_pos] = 0
                    mask_dict[k] = mask_dict[k].reshape(v.shape).to(tensor.device)
    
        return mask_dict
        
    
def find_good_params(model, train, keep_ratio, prune = True, largest = True, num_samples = len(train)):
    global chosen_params
    import random

    cuda_device = "cuda" if torch.cuda.is_available() else "cpu"

    param_dict = {}
    for name, param in model.named_parameters():
        param_dict[name] = torch.zeros_like(param).to(param.device)
            
    for i in range(0, num_samples):
        if 'qa' in train.columns.to_list():
            prompt = train.iloc[i]['qa']
        else:
            question = train['question'].iloc[i]
            answer = train['solution'].iloc[i]
            prompt = f"""Instruct: {question} Let's write a Python program.\nOutput:\n{answer}"""
        inputs = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
        outputs = model(inputs)
        for key, tensor in magnitude.items():
            try:
                param_dict[f"{key}.weight"] += tensor
            except:
                print(f'passed at {key}')
    keys_to_remove = [key for key in param_dict if key.split('.weight')[0] not in magnitude]

    for key in keys_to_remove:
        del param_dict[key]

    # create dictionary to store mask 
    mask_dict = {}


    for k, v in param_dict.items():
        # don't count classifier layer
        if "embed" in k:
            if prune == False:
                mask_dict[k] = torch.zeros_like(v).to(v.device)
            else:
                mask_dict[k] = torch.ones_like(v).to(v.device)

        else:
            if prune == False:
                sizes = v.shape
                num_params = v.numel()
                keep_num = int(num_params * keep_ratio)
                tensor = v.view(-1)
                top_pos = torch.topk(torch.abs(tensor), keep_num, largest = largest)[1]
                mask_dict[k] = torch.zeros_like(tensor, device=tensor.device)
                mask_dict[k][top_pos] = 1
                mask_dict[k] = mask_dict[k].reshape(v.shape).to(tensor.device)
            else:
                sizes = v.shape
                num_params = v.numel()
                keep_num = int(num_params * keep_ratio)
                tensor = v.view(-1)
                top_pos = torch.topk(torch.abs(tensor), keep_num, largest = largest)[1]
                mask_dict[k] = torch.ones_like(tensor, device=tensor.device)
                mask_dict[k][top_pos] = 0
                mask_dict[k] = mask_dict[k].reshape(v.shape).to(tensor.device)

    return mask_dict
    
def prune(bad_params, good_params, factor, return_good = False):
    prune_params = {}
    if return_good ==False:
        for k, v in bad_params.items():
            prune_params[k] = bad_params[k] - good_params[k]
            indices = prune_params[k]!=-1
            bad_indices = prune_params[k]==-1
            prune_params[k] = indices + (bad_indices*factor)

    else:
        for k, v in bad_params.items():
            prune_params[k] = good_params[k] - bad_params[k]
            indices = prune_params[k]!=-1
            good_indices = prune_params[k]==-1
            prune_params[k] = indices + (good_indices*factor)
    return prune_params

def scale(good_params, factor):
    prune_params = {}
    for k, v in good_params.items():
        good_indices = good_params[k]!=1
        keep_indices = good_params[k]==1
        prune_params[k] = keep_indices + (good_indices*factor)
    return prune_params

num_samples = args.num_samples
num_repeats = args.num_repeats
if args.proportion is None:
    #good_percents = [.0001, .001, .005, .01, .025, .05, .1, .15]
    good_percents = [0.0001, .001, .01, 0.1]
if args.proportion is not None:
    good_percents = [args.proportion]
scalar = args.scalar
for dataset in dataset_list:
    for repeat in range(0, num_repeats):
        run_seed = args.random_state + repeat
        sampled_train = train.sample(n = num_samples, replace = True, random_state = run_seed)
        sampled_comparison = dataset.sample(n = num_samples, replace = True, random_state = run_seed)
        for good_percent in good_percents:
            model = AutoModelForCausalLM.from_pretrained(args.model, device_map="auto", torch_dtype=torch.bfloat16)
            torch.cuda.empty_cache()
            magnitude = {}
            def getActivation(name):
                # The hook function
                def hook(module, input, output):
                    activations = input[0]  # Get the input activations
                    weights = module.weight.data  # Get the weights
                    device = weights.device
                    # Compute the norm of activations along dim=1
                    activations_norm = activations.norm(p=2, dim=1).to(torch.bfloat16)
                    # Multiply activations by the absolute value of weights
                    modified_output = activations_norm.to(device) * torch.abs(weights)
                    magnitude[name] = modified_output.detach()  # Store the modified output
                # Return the hook function
                return hook
            
            for name, module in model.named_modules():
                if (isinstance(module, (nn.Linear))):
                    hook_fn = getActivation(name)  # Get the hook function
                    module.register_forward_hook(hook_fn)  # Register the hook function
            good_params = find_good_params(model, sampled_train, keep_ratio=good_percent, prune = True, largest = True, num_samples = num_samples)
            torch.cuda.empty_cache()
            if 'Bad' in dataset.name:    
                comparison_params = find_params(model, sampled_comparison, keep_ratio=good_percent, prune = True, largest = True, num_samples = num_samples)
            else:
                comparison_params = find_good_params(model, sampled_comparison, keep_ratio=good_percent, prune = True, largest = True, num_samples = num_samples)

            prune_params = prune(comparison_params, good_params, scalar, return_good = True)
            del good_params
            del comparison_params
            for key, tensor in prune_params.items():
                device = model.state_dict()[key].device
                tensor = tensor.to(device)
                model.state_dict()[key]*=tensor
                
            del prune_params
            def remove_hooks(model):
                # Function to remove all hooks
                for name, module in model.named_modules():
                    # Check if the module has any forward hooks
                    if hasattr(module, "_forward_hooks") and len(module._forward_hooks) > 0:
                        # Remove all forward hooks
                        module._forward_hooks.clear()
        
            remove_hooks(model)
            if args.streetmath_eval and not args.eval_datasets and args.train_lm_eval_task is None:
                streetmath_out = f"{args.save_path}/eval_results/{args.model}/STREET_MATH_calculate{good_percent}_run{repeat}.jsonl"
                os.makedirs(os.path.dirname(streetmath_out), exist_ok=True)
                run_streetmath_eval(model, tokenizer, streetmath_out, args)
            if 'sgsm' in args.train_dataset:
                prune_solve = []
                prune_code = []
                prune_solutions = []
                for i in range(0, min(args.eval_dataset_subset, len(val))):
                    # Format the prompt
                    prompts = []
                    questions = []
                    final_question = val.iloc[i]['question']
                    final_answer = val.iloc[i]['answer']
                    final_prompt = f"""Instruct: {final_question} Let's write a Python program.\nOutput:"""
    
                    for j in range(0, 8):
                        question = train['question'].iloc[j]
                        questions.append(question)
                        answer = train['solution'].iloc[j]
                        prompt = f"""Instruct: {question} Let's write a Python program.\nOutput:\n{answer}"""
                        if prompt not in prompts:
                            prompts.append(prompt)
    
                    prompts.append(final_prompt)
                    formatted_prompt = "\n\n".join(prompts)
                    #Query the model 
                    inputs = tokenizer.encode(formatted_prompt, return_tensors="pt").to(model.device)
                    model_answer = None
                    #output = model.generate(inputs, max_new_tokens = 150, temperature = .7, do_sample = True)
                    output = model.generate(inputs, max_new_tokens = 150)
                    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
                    # Split the generated text by the prompt to extract the newly generated part
                    generated_text_parts = generated_text.split(final_prompt)
                    solution_text = generated_text_parts[-1].strip()
                    prune_solutions.append(solution_text)
                    if "Instruct:" in solution_text:
                        solution_text = solution_text.split("Instruct:")[0] # Split up a generation that contains more than one question
                    if "print" in solution_text:
                        solution_text = solution_text.split("print")[0] # Split up a generation that contains a print statement
                    if "Student:" in solution_text:
                        solution_text = solution_text.split("Student:")[0] # Split up a generation that contains more than one question
                    if "Output:" in solution_text:
                        solution_text = solution_text.split("Output:")[0] # Split up a generation that contains more than one question
                    if "#TODO" in solution_text:
                        solution_text = solution_text.split("#TODO")[0] # Split up a generation that contains more than one question
                    #solutions.append(solution_text)
                    if 'return result' in solution_text:
                        # Split the string on 'return result' but keep 'return result' in the result
                        parts = re.split(r'(return result)', solution_text)
    
                        # Rejoin the parts correctly
                        solution_text = parts[0] + parts[1]
                    try:
                        exec(solution_text)
                        model_answer = solution()
                        prune_code.append(1)
                        model_answer = float(model_answer)
                        if model_answer != final_answer:
                            prune_solve.append(0)
    
                        if model_answer == final_answer:
                            prune_solve.append(1)
    
                    except:
                        prune_code.append(0)
                        prune_solve.append(0)
    
                with open(output_file, "a") as f:  # Open the file in append mode ("a")
                        f.write(f"Average eval accuracy on {min(args.eval_dataset_subset, len(val))} questions for pruning top {good_percent}% good parameters based on not being activated by {dataset.name} based on {num_samples} training samples and greedy decoding (few-shot): {np.mean(prune_solve)}\n")  
                torch.cuda.empty_cache()
                task_manager = build_task_manager(task_manager_names)
                #--log_samples --output_path results/phi_15_base --device cuda:0 --batch_size auto:4
                # Setting `task_manager` to the one above is optional and should generally be done
                # if you want to include tasks from paths other than ones in `lm_eval/tasks`.
                # `simple_evaluate` will instantiate its own task_manager if it is set to None here.
                results = lm_eval.simple_evaluate( # call simple_evaluate
                    model = 'hf',
                    model_args = {'pretrained':model, 'dtype': 'bfloat16', 'tokenizer': tokenizer},
                    tasks=args.eval_datasets,
                    task_manager=task_manager,
                    log_samples = False, 
                    batch_size = 'auto:4',
                    limit = args.eval_dataset_subset,
                    random_seed = run_seed
                )
                results_path = f"{args.save_path}/eval_results/{args.model}/{dataset.name}_calculate{good_percent}_run{repeat}.json"
                os.makedirs(os.path.dirname(results_path), exist_ok=True)
                with open(results_path, "w") as outfile: 
                    json.dump(results['results'], outfile)
                if args.streetmath_eval:
                    streetmath_out = f"{args.save_path}/eval_results/{args.model}/STREET_MATH_calculate{good_percent}_run{repeat}.jsonl"
                    run_streetmath_eval(model, tokenizer, streetmath_out, args)
            if args.train_lm_eval_task is not None:
                task_manager = build_task_manager(task_manager_names)
                #--log_samples --output_path results/phi_15_base --device cuda:0 --batch_size auto:4
                # Setting `task_manager` to the one above is optional and should generally be done
                # if you want to include tasks from paths other than ones in `lm_eval/tasks`.
                # `simple_evaluate` will instantiate its own task_manager if it is set to None here.
                results = lm_eval.simple_evaluate( # call simple_evaluate
                    model = 'hf',
                    model_args = {'pretrained':model, 'dtype': 'bfloat16', 'tokenizer': tokenizer},
                    tasks=[args.train_lm_eval_task],
                    task_manager=task_manager,
                    log_samples = False, 
                    batch_size = 'auto:4',
                    limit = args.eval_dataset_subset, 
                    random_seed = run_seed
                )
                results_path = f"{args.save_path}/eval_results/{args.model}/{args.train_lm_eval_task}_calculate{good_percent}_run{repeat}_train_task.json"
                os.makedirs(os.path.dirname(results_path), exist_ok=True)
                with open(results_path, "w") as outfile: 
                    json.dump(results['results'], outfile)
                    
                results = lm_eval.simple_evaluate( # call simple_evaluate
                    model = 'hf',
                    model_args = {'pretrained':model, 'dtype': 'bfloat16', 'tokenizer': tokenizer},
                    tasks=args.eval_datasets,
                    task_manager=task_manager,
                    log_samples = False, 
                    batch_size = 'auto:4',
                    limit = args.eval_dataset_subset,
                    random_seed = run_seed
                )
                results_path = f"{args.save_path}/eval_results/{args.model}/{dataset.name}_calculate{good_percent}_run{repeat}.json"
                os.makedirs(os.path.dirname(results_path), exist_ok=True)
                with open(results_path, "w") as outfile: 
                    json.dump(results['results'], outfile)
                if args.streetmath_eval:
                    streetmath_out = f"{args.save_path}/eval_results/{args.model}/STREET_MATH_calculate{good_percent}_run{repeat}.jsonl"
                    run_streetmath_eval(model, tokenizer, streetmath_out, args)
                        
            del model
